{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d44a27",
   "metadata": {
    "editable": false,
    "id": "49603f07",
    "lang": "fr",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Devoir 3, Question 3 : Discriminant avec noyau et descente du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a17b1a",
   "metadata": {
    "editable": false,
    "id": "e36ddca2",
    "lang": "en",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Homework 3, Question 3: Kernel discriminant and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb7d03",
   "metadata": {
    "editable": false,
    "id": "a728e2e6",
    "lang": "fr",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Code préambule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a661e",
   "metadata": {
    "editable": false,
    "id": "1e202079",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Preamble code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f56a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-18T04:29:02.317763Z",
     "start_time": "2021-12-18T04:29:01.342576Z"
    },
    "editable": false,
    "id": "508a9904",
    "tags": [
     "problem-context",
     "autoexec"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import itertools\n",
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', 0)\n",
    "import collections\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# Nous ne voulons pas être signalé par ce type d'avertissement, non pertinent pour le devoir\n",
    "# We don't want to be signaled of this warning, irrelevant for the homework\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Fonction pour vérifier le temps d'exécution\n",
    "# Function to verify execution time\n",
    "_times = []\n",
    "def checkTime(maxduration, question):\n",
    "    duration = _times[-1] - _times[-2]\n",
    "    if duration > maxduration:\n",
    "        print(\"[ATTENTION] Votre code pour la question {0} met trop de temps à s'exécuter! \".format(question)+\n",
    "              \"Le temps maximum permis est de {0:.4f} secondes, \".format(maxduration)+\n",
    "              \"mais votre code a requis {0:.4f} secondes! \".format(duration)+\n",
    "              \"Assurez-vous que vous ne faites pas d'appels bloquants (par exemple à show()) dans cette boucle!\")\n",
    "\n",
    "# Définition des durées d'exécution maximales pour chaque classifieur\n",
    "# Definition of maximum execution time for each classifier\n",
    "TMAX_EVAL = 3.0\n",
    "TMAX_FIT = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610261d",
   "metadata": {
    "editable": false,
    "id": "80706621",
    "lang": "fr",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "Soit un discriminant avec noyau gaussien, entraîné par descente du gradient avec la fonction\n",
    "d’erreur suivante :\n",
    "$$E(\\mathbf{\\alpha},w_0|\\mathcal{X})=\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t},$$\n",
    "avec :\n",
    "$$\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})=\\sum_{\\mathbf{x}^{s}\\in\\mathcal{X}}{\\alpha^{s}r^{s}}K(\\mathbf{x}^{s},\\mathbf{x}^{t})+w_{0},$$\n",
    "$$\\mathcal{Y}=\\{\\mathbf{x}^{t}\\in\\mathcal{X}\\,|\\,r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})<1\\},$$\n",
    "$$\\alpha^{t}\\geq 0,~\\forall t.$$\n",
    "\n",
    "- Les valeurs cibles sont dans $r^{t}\\in\\{−1,1\\}$;\n",
    "- Le classement se fait selon le signe de $\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})$;\n",
    "- Le paramètre $\\lambda$ permet de contrôler le niveau de régularisation effectué."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bf47e",
   "metadata": {
    "editable": false,
    "id": "60090dc1",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "Consider a discriminant with a Gaussian kernel, trained by gradient descent with the following error function:\n",
    "$$E(\\mathbf{\\alpha},w_0|\\mathcal{X})=\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t},$$\n",
    "with:\n",
    "$$\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})=\\sum_{\\mathbf{x}^{s}\\in\\mathcal{X}}{\\alpha^{s}r^{s}}K(\\mathbf{x}^{s},\\mathbf{x}^{t})+w_{0},$$\n",
    "$$\\mathcal{Y}=\\{\\mathbf{x}^{t}\\in\\mathcal{X}\\,|\\,r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})<1\\},$$\n",
    "$$\\alpha^{t}\\geq 0,~\\forall t.$$\n",
    "\n",
    "- The target values are in $r^{t}\\in\\{−1,1\\}$;\n",
    "- The classification is done according to the sign of $\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})$;\n",
    "- The $\\lambda$ parameter is used to control the level of regularization performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0920f0",
   "metadata": {
    "editable": false,
    "id": "59129adf",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "## Q3A\n",
    "Développez les équations des gradients des poids $\\alpha^{t}$ et de la constante $w_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edb484",
   "metadata": {
    "editable": false,
    "id": "5a30d679",
    "lang": "en",
    "tags": []
   },
   "source": [
    "## Q3A\n",
    "Develop the equations of the gradients of the weights $\\alpha^{t}$ and the constant $w_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d245d15",
   "metadata": {
    "editable": false,
    "id": "c794f780",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Entrez votre solution à Q3A dans la cellule ci-dessous (markdown et $\\LaTeX$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2ccf6",
   "metadata": {
    "editable": false,
    "id": "92c46989",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Enter your answer to Q3A in the cell below (markdown and $\\LaTeX$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c457f",
   "metadata": {
    "editable": false,
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "\n",
    "        <div class=\"feedback-cell\" style=\"background: rgba(100 , 100 , 100 , 0.4)\">\n",
    "            <h3>Votre soumission a été enregistrée!</h3>\n",
    "            <ul>\n",
    "                <li>notez qu'il n'y a <strong>pas</strong> de correction automatique pour cet exercice&puncsp;;</li>\n",
    "                <li>par conséquent, votre note est <strong>actuellement</strong> zéro&puncsp;;</li>\n",
    "                <li>elle sera cependant ajustée par le professeur dès que la correction manuelle sera complétée&puncsp;;</li>\n",
    "                <li>vous pouvez soumettre autant de fois que nécessaire jusqu'à la date d'échéance&puncsp;;</li>\n",
    "                <li>mais évitez de soumettre inutilement.</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316682d3",
   "metadata": {
    "deletable": false,
    "id": "1b67c907",
    "tags": [
     "user-answer-D3Q3A",
     "editable"
    ]
   },
   "source": [
    "$\\begin{align*} \n",
    "E(\\mathbf{\\alpha},w_0|\\mathcal{X}) \n",
    "&= \\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t}\n",
    "\\end{align*}$\n",
    "\n",
    "Descente du gradient:\\\n",
    "$\\begin{align*} \n",
    "\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial\\mathbf{\\alpha}^*}\n",
    "&= \\frac{\\partial}{\\partial\\mathbf{\\alpha}^*} \\bigg(\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t}\\bigg) \\\\\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\frac{\\partial r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})}{\\partial\\mathbf{\\alpha}^*} +  \\lambda\\frac{\\partial\\sum_{t=1}^N\\alpha^{t}}{\\partial\\mathbf{\\alpha}^*} \\\\\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\frac{\\partial r^{t}\\mathrm{h}}{\\partial\\mathrm{h}}\\frac{\\partial \\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})}{\\partial\\mathbf{\\alpha}^*} +  \\lambda\\frac{\\partial\\sum_{t=1}^N\\alpha^{t}}{\\partial\\mathbf{\\alpha}^*} \\\\\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}r^{*}K(\\mathbf{x}^{*},\\mathbf{x}^{t})  +  \\lambda\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*} \n",
    "\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial w_0}\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}\n",
    "\\end{align*}$\n",
    "\n",
    "On a donc: \\\n",
    "$\\mathbf{\\Delta}\\mathbf{\\alpha}^t \n",
    "= -\\eta\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial\\mathbf{\\alpha}^t}\n",
    "= \\eta\\big(\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}r^{*}K(\\mathbf{x}^{*},\\mathbf{x}^{t})  -  \\lambda\\big)$\n",
    "\n",
    "$\\mathbf{\\Delta}w_0 = \n",
    "= -\\eta\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial w_0}\n",
    "= \\eta\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}$\n",
    "\n",
    "Mise à jour des poids $\\mathbf{\\alpha}^t$ et $w_0$: \\\n",
    "$\\mathbf{\\alpha}^t = \n",
    "\\begin{cases}\n",
    "0 & si & \\mathbf{\\alpha}^t + \\mathbf{\\Delta}\\mathbf{\\alpha}^t < 0 \\\\\n",
    "\\mathbf{\\alpha}^t + \\mathbf{\\Delta}\\mathbf{\\alpha}^t & autrement\n",
    "\\end{cases}$\n",
    "\n",
    "$w_0 = w_0 + \\mathbf{\\Delta}w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32ce24",
   "metadata": {
    "editable": false,
    "id": "80da7504",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "## Q3B\n",
    "Implémentez ce classifieur en respectant l’interface scikit-learn, en donnant au minimum les fonctions `fit`, `predict` et `score`. Pour l’optimisation des paramètres du classifieur, utilisez la méthode L-BFGS disponible dans SciPy (`scipy.optimize.fmin_l_bfgs_b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e12461",
   "metadata": {
    "editable": false,
    "id": "b6175747",
    "lang": "en",
    "tags": []
   },
   "source": [
    "## Q3B\n",
    "Implement this classifier while respecting the scikit-learn interface, giving at least the functions `fit`, `predict` and `score`. For the optimization of the classifier parameters, use the L-BFGS method available in SciPy (`scipy.optimize.fmin_l_bfgs_b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cd105",
   "metadata": {
    "editable": false,
    "id": "f48c8818",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0564d",
   "metadata": {
    "editable": false,
    "id": "49a088e9",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3B answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c2ad3",
   "metadata": {
    "editable": false,
    "id": "d9402224",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Implémentation du discriminant à noyau\n",
    "# Kernel discriminant implementation\n",
    "class DiscriminantANoyau:\n",
    "    \n",
    "    def __init__(self, lambda_, sigma, verbose=True):\n",
    "        # Cette fonction est déjà codée pour vous, vous n'avez qu'à utiliser les variables membres qu'elle \n",
    "        # définit dans les autres fonctions de cette classe. Lambda et sigma sont définis dans l'énoncé.\n",
    "        # verbose permet d'afficher certaines statistiquesen lien avec la convergence de fmin_l_bfgs_b.\n",
    "        # This function is already coded for you, you just have to use the member variables it defines in \n",
    "        # in the other functions of this class. Lambda and sigma are defined in the statement.\n",
    "        # verbose allows you to display some statistics related to the convergence of the fmin_l_bfgs_b.\n",
    "        self.lambda_ = lambda_\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Implémentez la fonction d'entraînement du classifieur, selon les équations développées à Q3A\n",
    "        # Implement the training function of the classifier, according to the equations developed in Q3A\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Vous devez écrire une fonction nommée evaluateFunc, qui reçoit un seul argument, soit les valeurs\n",
    "        # des paramètres pour lesquels on souhaite connaître l'erreur et le gradient. Cette fonction sera \n",
    "        # appelée à répétition par l'optimiseur de scipy, qui l'utilisera pour minimiser l'erreur et \n",
    "        # obtenir un jeu de paramètres optimal.\n",
    "        # You must write a function named evaluateFunc, which receives only one argument, either the values\n",
    "        # of the parameters for which you want to know the error and the gradient. This function will be \n",
    "        # called repeatedly by the scipy optimizer, which will use it to minimize the error and \n",
    "        # to obtain an optimal set of parameters.\n",
    "        def evaluateFunc(params):\n",
    "            # Ecrire ici le code de la fonction evaluateFunc calculant l'erreur \n",
    "            # et le gradient de params.\n",
    "            # Write here the code of the evaluateFunc function calculating the error \n",
    "            # and the gradient of params.\n",
    "            return err, grad\n",
    "        # ******\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Initialisez aléatoirement les paramètres alpha^t et w_0 (l'optimiseur requiert une valeur initiale, \n",
    "        # et nous ne pouvons pas simplement n'utiliser que des zéros pour différentes raisons). Stockez ces \n",
    "        # valeurs initiales aléatoires dans un array numpy nommé \"params\". Déterminez également les bornes à\n",
    "        # utiliser sur ces paramètres et stockez les dans une variable nommée \"bounds\".\n",
    "        # Indice : les paramètres peuvent-ils avoir une valeur maximale (au-dessus de laquelle ils ne veulent\n",
    "        # plus rien dire)? Une valeur minimale? Référez-vous à la documentation de fmin_l_bfgs_b pour savoir\n",
    "        # comment indiquer l'absence de bornes.\n",
    "        # Randomly initialize the parameters alpha^t and w_0 (the optimizer requires an initial value, \n",
    "        # and we can't just use zeros for various reasons). Store these \n",
    "        # random initial values in a numpy array named \"params\". Also determine the bounds to use on these\n",
    "        # parameters and store them in a variable named \"bounds\".\n",
    "        # Hint: Can the parameters have a maximum value (above which they mean nothing)?\n",
    "        # mean anything)? A minimum value? Refer to the documentation of fmin_l_bfgs_b to know\n",
    "        # how to indicate the absence of bounds.\n",
    "        # ******\n",
    "\n",
    "        # À ce stade, trois choses devraient être définies :\n",
    "        # - Une fonction d'évaluation nommée evaluateFunc, capable de retourner l'erreur et le gradient d'erreur \n",
    "        #   pour chaque paramètre, pour une configuration d'alpha et w_0 donnée.\n",
    "        # - Un tableau numpy nommé params de même taille que le nombre de paramètres à entraîner.\n",
    "        # - Une liste nommée bounds contenant les bornes que l'optimiseur doit respecter pour chaque paramètre.\n",
    "        # On appelle maintenant l'optimiseur avec ces informations et on conserve les valeurs dans params\n",
    "        # At this point, three things should be defined:\n",
    "        # - An evaluation function named evaluateFunc, capable of returning the error and error gradient \n",
    "        # for each parameter, for a given alpha and w_0 configuration.\n",
    "        # - A numpy array named params of the same size as the number of parameters to train.\n",
    "        # - A list named bounds containing the bounds that the optimizer must respect for each parameter.\n",
    "        # We now call the optimizer with this information and we keep the values in params\n",
    "        _times.append(time.time())\n",
    "        params, minval, infos = fmin_l_bfgs_b(evaluateFunc, params, bounds=bounds)\n",
    "        _times.append(time.time())\n",
    "        checkTime(TMAX_FIT, \"Entrainement\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            # On affiche quelques statistiques / display some statistics\n",
    "            print(\"Entraînement terminé après {it} itérations et {calls} appels à evaluateFunc\".format(it=infos['nit'], calls=infos['funcalls']))\n",
    "            print(\"\\tErreur minimale : {:.5f}\".format(minval))\n",
    "            print(\"\\tL'algorithme a convergé\" if infos['warnflag'] == 0 else \"\\tL'algorithme n'a PAS convergé\")\n",
    "            print(\"\\tGradients des paramètres à la convergence (ou à l'épuisement des ressources) :\")\n",
    "            print(infos['grad'])\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Stockez les paramètres optimisés de la façon suivante :\n",
    "        # - Le vecteur alpha dans self.alphas;\n",
    "        # - Le biais w_0 dans self.w0.\n",
    "        # Store the optimized parameters as follows:\n",
    "        # - The alpha vector in self.alphas;\n",
    "        # - The bias w_0 in self.w0.\n",
    "        # ******\n",
    "\n",
    "        # On retient également le jeu d'entraînement, qui pourra vous être utile pour les autres fonctions.\n",
    "        # We also retain the training set, which can be useful for other functions.\n",
    "        self.X, self.y = X, y\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentez la fonction d'inférence (prédiction). Vous pouvez supposer que fit() a préalablement été\n",
    "        # exécuté et que les variables membres alphas, w0, X et y existent. N'oubliez pas que ce classifieur doit\n",
    "        # retourner -1 ou 1\n",
    "        # Implement the inference (prediction) function. You can assume that fit() has been previously executed and that the\n",
    "        # executed and that the member variables alphas, w0, X and y exist. Remember that this classifier must\n",
    "        # return -1 or 1\n",
    "        # ******\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentez la fonction retournant le score (accuracy) du classifieur sur les données reçues en\n",
    "        # argument. Vous pouvez supposer que fit() a préalablement été exécutée\n",
    "        # Indice : réutiliser votre implémentation de predict() réduit de beaucoup la taille de cette fonction!\n",
    "        # Implement the function returning the accuracy of the classifier on the data received as\n",
    "        # argument. You can assume that fit() has already been executed\n",
    "        # Hint: reusing your implementation of predict() reduces the size of this function by a lot!\n",
    "        # ******\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87edb73",
   "metadata": {
    "editable": false,
    "id": "0381df3e",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Entrez votre solution à Q3B dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294eeca9",
   "metadata": {
    "editable": false,
    "id": "6238b53f",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Enter your answer to Q3B in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce725f",
   "metadata": {
    "deletable": false,
    "id": "1ec4df02",
    "tags": [
     "user-answer-D3Q3B",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Implémentation du discriminant à noyau\n",
    "# Kernel discriminant implementation\n",
    "class DiscriminantANoyau:\n",
    "    \n",
    "    def __init__(self, lambda_, sigma, verbose=True):\n",
    "        # Cette fonction est déjà codée pour vous, vous n'avez qu'à utiliser les variables membres qu'elle \n",
    "        # définit dans les autres fonctions de cette classe. Lambda et sigma sont définis dans l'énoncé.\n",
    "        # verbose permet d'afficher certaines statistiquesen lien avec la convergence de fmin_l_bfgs_b.\n",
    "        # This function is already coded for you, you just have to use the member variables it defines in \n",
    "        # in the other functions of this class. Lambda and sigma are defined in the statement.\n",
    "        # verbose allows you to display some statistics related to the convergence of the fmin_l_bfgs_b.\n",
    "        self.lambda_ = lambda_\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose \n",
    "        \n",
    "    def kernel(self, a, b):\n",
    "        return numpy.exp(-cdist(a, b)**2 / self.sigma**2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Implémentez la fonction d'entraînement du classifieur, selon les équations développées à Q3A\n",
    "        # Implement the training function of the classifier, according to the equations developed in Q3A\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Vous devez écrire une fonction nommée evaluateFunc, qui reçoit un seul argument, soit les valeurs\n",
    "        # des paramètres pour lesquels on souhaite connaître l'erreur et le gradient. Cette fonction sera \n",
    "        # appelée à répétition par l'optimiseur de scipy, qui l'utilisera pour minimiser l'erreur et \n",
    "        # obtenir un jeu de paramètres optimal.\n",
    "        # You must write a function named evaluateFunc, which receives only one argument, either the values\n",
    "        # of the parameters for which you want to know the error and the gradient. This function will be \n",
    "        # called repeatedly by the scipy optimizer, which will use it to minimize the error and \n",
    "        # to obtain an optimal set of parameters.\n",
    "        def evaluateFunc(params):\n",
    "            # Ecrire ici le code de la fonction evaluateFunc calculant l'erreur \n",
    "            # et le gradient de params.\n",
    "            # Write here the code of the evaluateFunc function calculating the error \n",
    "            # and the gradient of params.\n",
    "            \n",
    "            w0 = params[0]\n",
    "            alphas = params[1:]\n",
    "            \n",
    "            hypothesis = numpy.sum(self.kernel(X, X) * y *  alphas, axis=1) + w0\n",
    "            f = numpy.where(y.reshape(-1) * hypothesis.reshape(-1) < 1, True, False).reshape(-1)\n",
    "\n",
    "            err = numpy.sum(1 - y[f] * hypothesis[f]) + self.lambda_ * numpy.sum(alphas)\n",
    "            \n",
    "            delta_alphas =  y * numpy.sum(y[f] * self.kernel(X, X[f, :])) - self.lambda_\n",
    "            delta_w0 = numpy.sum(y[f])\n",
    "            grad = numpy.concatenate(([delta_w0], delta_alphas.reshape(-1)))\n",
    "            \n",
    "            return err, grad\n",
    "        # ******\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Initialisez aléatoirement les paramètres alpha^t et w_0 (l'optimiseur requiert une valeur initiale, \n",
    "        # et nous ne pouvons pas simplement n'utiliser que des zéros pour différentes raisons). Stockez ces \n",
    "        # valeurs initiales aléatoires dans un array numpy nommé \"params\". Déterminez également les bornes à\n",
    "        # utiliser sur ces paramètres et stockez les dans une variable nommée \"bounds\".\n",
    "        # Indice : les paramètres peuvent-ils avoir une valeur maximale (au-dessus de laquelle ils ne veulent\n",
    "        # plus rien dire)? Une valeur minimale? Référez-vous à la documentation de fmin_l_bfgs_b pour savoir\n",
    "        # comment indiquer l'absence de bornes.\n",
    "        # Randomly initialize the parameters alpha^t and w_0 (the optimizer requires an initial value, \n",
    "        # and we can't just use zeros for various reasons). Store these \n",
    "        # random initial values in a numpy array named \"params\". Also determine the bounds to use on these\n",
    "        # parameters and store them in a variable named \"bounds\".\n",
    "        # Hint: Can the parameters have a maximum value (above which they mean nothing)?\n",
    "        # mean anything)? A minimum value? Refer to the documentation of fmin_l_bfgs_b to know\n",
    "        # how to indicate the absence of bounds.\n",
    "        # ******\n",
    "        params = numpy.random.rand(X.shape[0] + 1)\n",
    "        params = params.reshape((params.shape[0], 1)) \n",
    "        \n",
    "        bounds = numpy.array([(0, None)]*X.shape[0] + [(None, None)])\n",
    "\n",
    "        # À ce stade, trois choses devraient être définies :\n",
    "        # - Une fonction d'évaluation nommée evaluateFunc, capable de retourner l'erreur et le gradient d'erreur \n",
    "        #   pour chaque paramètre, pour une configuration d'alpha et w_0 donnée.\n",
    "        # - Un tableau numpy nommé params de même taille que le nombre de paramètres à entraîner.\n",
    "        # - Une liste nommée bounds contenant les bornes que l'optimiseur doit respecter pour chaque paramètre.\n",
    "        # On appelle maintenant l'optimiseur avec ces informations et on conserve les valeurs dans params\n",
    "        # At this point, three things should be defined:\n",
    "        # - An evaluation function named evaluateFunc, capable of returning the error and error gradient \n",
    "        # for each parameter, for a given alpha and w_0 configuration.\n",
    "        # - A numpy array named params of the same size as the number of parameters to train.\n",
    "        # - A list named bounds containing the bounds that the optimizer must respect for each parameter.\n",
    "        # We now call the optimizer with this information and we keep the values in params\n",
    "        _times.append(time.time())\n",
    "        params, minval, infos = fmin_l_bfgs_b(evaluateFunc, params, bounds=bounds)\n",
    "        _times.append(time.time())\n",
    "        checkTime(TMAX_FIT, \"Entrainement\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            # On affiche quelques statistiques / display some statistics\n",
    "            print(\"Entraînement terminé après {it} itérations et {calls} appels à evaluateFunc\".format(it=infos['nit'], calls=infos['funcalls']))\n",
    "            print(\"\\tErreur minimale : {:.5f}\".format(minval))\n",
    "            print(\"\\tL'algorithme a convergé\" if infos['warnflag'] == 0 else \"\\tL'algorithme n'a PAS convergé\")\n",
    "            print(\"\\tGradients des paramètres à la convergence (ou à l'épuisement des ressources) :\")\n",
    "            print(infos['grad'])\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Stockez les paramètres optimisés de la façon suivante :\n",
    "        # - Le vecteur alpha dans self.alphas;\n",
    "        # - Le biais w_0 dans self.w0.\n",
    "        # Store the optimized parameters as follows:\n",
    "        # - The alpha vector in self.alphas;\n",
    "        # - The bias w_0 in self.w0.\n",
    "        # ******\n",
    "        self.alphas = params[1:]\n",
    "        self.w0 = params[0]\n",
    "\n",
    "        # On retient également le jeu d'entraînement, qui pourra vous être utile pour les autres fonctions.\n",
    "        # We also retain the training set, which can be useful for other functions.\n",
    "        self.X, self.y = X, y\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentez la fonction d'inférence (prédiction). Vous pouvez supposer que fit() a préalablement été\n",
    "        # exécuté et que les variables membres alphas, w0, X et y existent. N'oubliez pas que ce classifieur doit\n",
    "        # retourner -1 ou 1\n",
    "        # Implement the inference (prediction) function. You can assume that fit() has been previously executed and that the\n",
    "        # executed and that the member variables alphas, w0, X and y exist. Remember that this classifier must\n",
    "        # return -1 or 1\n",
    "        # ******\n",
    "        hypothesis = numpy.sum(self.kernel(X, self.X) * self.y * self.alphas.reshape(-1), axis=1) + self.w0\n",
    "        predictions = numpy.where(hypothesis >= 0, 1, -1)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Implémentez la fonction retournant le score (accuracy) du classifieur sur les données reçues en\n",
    "        # argument. Vous pouvez supposer que fit() a préalablement été exécutée\n",
    "        # Indice : réutiliser votre implémentation de predict() réduit de beaucoup la taille de cette fonction!\n",
    "        # Implement the function returning the accuracy of the classifier on the data received as\n",
    "        # argument. You can assume that fit() has already been executed\n",
    "        # Hint: reusing your implementation of predict() reduces the size of this function by a lot!\n",
    "        # ******\n",
    "        score = numpy.sum(numpy.where(y * self.predict(X) >= 0, 1, 0)) / X.shape[0]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482aa754",
   "metadata": {
    "editable": false,
    "id": "96e3d292",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "## Q3C\n",
    "Expérimentez avec le classifieur développé à la question précédente (Q3B) en utilisant un jeu de 800 données synthétiques *moons* selon deux classes et avec du bruit blanc, soit $\\sigma_{bruit}=0.3$ avec $datasets.make\\_moons(n\\_samples=800, noise=0.3)$.\n",
    "\n",
    "Divisez le jeu de données en deux parties de taille égale, une pour l’entraînement et une pour le test. Affichez les données ainsi que la frontière obtenue avec le classifieur pour une paramétrisation de $\\lambda$ et $\\sigma$ (étalement du noyau gaussien) permettant d’obtenir un taux d’erreur inférieur à 10 % en évaluation sur le jeu de test. Pour obtenir un taux d’erreur inférieur à 10 % sur le jeu de données moons, il vous faudra probablement resserrer la recherche en grille sur le paramètre $\\sigma\\in\\left[0,1\\right]$.\n",
    "\n",
    "Pour cette question, donnez :\n",
    "- Les différents paramètres d’entraînement évalués par la recherche en grille ainsi que les performances associées;\n",
    "- Les valeurs de paramètres finaux retenus ainsi que les performances correspondantes (entraînement et test);\n",
    "- Le graphique des frontières de décision de la configuration retenue et des données utilisées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e4915",
   "metadata": {
    "editable": false,
    "id": "a96a21ee",
    "lang": "en",
    "tags": []
   },
   "source": [
    "## Q3C\n",
    "Experiment with the classifier developed at the previous question (Q3B) using a set of 800 synthetic data *moons* according to two classes and with white noise, that is $\\sigma_{bruit}=0.3$ with $datasets.make\\_moons(n\\_samples=800, noise=0.3)$.\n",
    "\n",
    "Divide the dataset into two equally sized parts, one for training and one for testing. Display the data as well as the frontier obtained with the classifier for a parameterization of $\\lambda$ and $\\sigma$ (Gaussian kernel spreading) allowing to obtain an error rate of less than 10% in evaluation on the test set. To get an error rate of less than 10% on the moons dataset, you will probably need to tighten the grid search on the $\\sigma\\in\\left[0,1\\right]$ parameter.\n",
    "\n",
    "For this question, give:\n",
    "- The various training parameters evaluated by grid research as well as the associated performances;\n",
    "- The final parameter values retained as well as the corresponding performances (training and test);\n",
    "- The graph of the decision boundaries of the configuration selected and the data used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924f079",
   "metadata": {
    "editable": false,
    "id": "09a66670",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code réponse à l'exercice Q3C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa40ad",
   "metadata": {
    "editable": false,
    "id": "5de98b15",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3C answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a2a6d",
   "metadata": {
    "editable": false,
    "id": "be19f805",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Création du tableau pour accumuler les résultats\n",
    "# Creation of the table to accumulate the results\n",
    "results = {'Classifier':[\"DiscriminantANoyau\"],\n",
    "           'Range_lambda':[],\n",
    "           'Range_sigma':[],\n",
    "           'Best_lambda':[],\n",
    "           'Best_sigma':[],\n",
    "           'Error_train':[],\n",
    "           'Error_test':[],\n",
    "          }\n",
    "\n",
    "# *** TODO ***\n",
    "# Créez le jeu de données à partir de la fonction make_moons, tel que demandé dans l'énoncé\n",
    "# N'oubliez pas de vous assurer que les valeurs possibles de y sont bel et bien dans -1 et 1, et non 0 et 1!\n",
    "# Create the dataset from the make_moons function, as requested in the statement\n",
    "# Remember to make sure that the possible values of y are in -1 and 1, not 0 and 1!\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Séparez le jeu de données en deux parts égales, l'une pour l'entraînement et l'autre pour le test.\n",
    "# Separate the dataset into two equal parts, one for training and one for testing.\n",
    "# ******\n",
    "\n",
    "_times.append(time.time())\n",
    "    \n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# paramètre Lambda en la mettant dans \n",
    "# la liste de la variable range_lambda.\n",
    "# Specify the search range for the \n",
    "# parameter Lambda by putting it in \n",
    "# the list of the variable range_lambda.\n",
    "range_lambda = []\n",
    "# ******\n",
    "\n",
    "results['Range_lambda'].append(range_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# paramètre Sigma en la mettant dans \n",
    "# la liste de la variable range_sigma.\n",
    "# Specify the search range for the \n",
    "# parameter Sigma by putting it in \n",
    "# the list of the variable range_sigma.\n",
    "range_sigma = []\n",
    "# ******\n",
    "\n",
    "results['Range_sigma'].append(range_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Optimisez ici les paramètres lambda et sigma de votre \n",
    "# classifieur en effectuant une recherche en grille.\n",
    "# Optimize here the lambda and sigma parameters of your \n",
    "# classifier by performing a grid search.\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# paramètre Lambda en la mettant dans la\n",
    "# variable best_lambda en remplaçant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Lambda by putting it in the\n",
    "# variable best_lambda by replacing the 0.\n",
    "best_lambda = 0\n",
    "# ******\n",
    "\n",
    "results['Best_lambda'].append(best_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# paramètre Sigma en la mettant dans la\n",
    "# variable best_sigma en remplaçant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Sigma by putting it in the\n",
    "# variable best_sigma by replacing the 0.\n",
    "best_sigma = 0\n",
    "# ******\n",
    "\n",
    "results['Best_sigma'].append(best_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Une fois les paramètres lambda et\n",
    "# sigma de votre classifieur optimisés,\n",
    "# créez une instance de ce classifieur\n",
    "# en utilisant ces paramètres optimaux.\n",
    "# Once the lambda and sigma parameters of your classifier are optimized,\n",
    "# create an instance of this classifier\n",
    "# using these optimal parameters.\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Indicate the error rate obtained on the training set in the variable\n",
    "# err_train variable by replacing the 0.\n",
    "err_train = 0\n",
    "# ******\n",
    "\n",
    "results['Error_train'].append(err_train)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez le taux d'erreur obtenu sur le\n",
    "# jeu de test dans la variable \n",
    "# err_test en remplaçant le 0.\n",
    "# Indicate the error rate obtained on the\n",
    "# test set in the err_test variable by replacing the 0.\n",
    "err_test = 0\n",
    "# ******\n",
    "\n",
    "results['Error_test'].append(err_test)\n",
    "\n",
    "# *** TODO ***\n",
    "# Créez ici une grille permettant d'afficher les régions de décision pour chaque classifieur\n",
    "# Indice : numpy.meshgrid pourrait vous être utile ici\n",
    "# Par la suite, affichez les régions de décision dans la même figure que les données de test.\n",
    "# Note : un pas de 0.02 pour le meshgrid est recommandé\n",
    "# ******\n",
    "\n",
    "# On affiche la figure\n",
    "_times.append(time.time())\n",
    "checkTime(TMAX_EVAL, \"Evaluation\")\n",
    "pyplot.show()\n",
    "\n",
    "# Affichage des résultats\n",
    "df = pandas.DataFrame(results)\n",
    "display.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c6d10",
   "metadata": {
    "editable": false,
    "id": "eaf02ea0",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Entrez votre solution à Q3C dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516b95f",
   "metadata": {
    "editable": false,
    "id": "a22c170c",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Enter your answer to Q3C in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c984d32",
   "metadata": {
    "deletable": false,
    "id": "bbb930e4",
    "tags": [
     "user-answer-D3Q3C",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Création du tableau pour accumuler les résultats\n",
    "# Creation of the table to accumulate the results\n",
    "results = {'Classifier':[\"DiscriminantANoyau\"],\n",
    "           'Range_lambda':[],\n",
    "           'Range_sigma':[],\n",
    "           'Best_lambda':[],\n",
    "           'Best_sigma':[],\n",
    "           'Error_train':[],\n",
    "           'Error_test':[],\n",
    "          }\n",
    "\n",
    "# *** TODO ***\n",
    "# Créez le jeu de données à partir de la fonction make_moons, tel que demandé dans l'énoncé\n",
    "# N'oubliez pas de vous assurer que les valeurs possibles de y sont bel et bien dans -1 et 1, et non 0 et 1!\n",
    "# Create the dataset from the make_moons function, as requested in the statement\n",
    "# Remember to make sure that the possible values of y are in -1 and 1, not 0 and 1!\n",
    "# ******\n",
    "X, y = make_moons(𝑛_𝑠𝑎𝑚𝑝𝑙𝑒𝑠=800, 𝑛𝑜𝑖𝑠𝑒=0.3)\n",
    "y = numpy.where(y==0, -1, 1)\n",
    "\n",
    "# *** TODO ***\n",
    "# Séparez le jeu de données en deux parts égales, l'une pour l'entraînement et l'autre pour le test.\n",
    "# Separate the dataset into two equal parts, one for training and one for testing.\n",
    "# ******\n",
    "X1, X2, y1, y2 = train_test_split(X, y, train_size=0.5, random_state=96)\n",
    "\n",
    "_times.append(time.time())\n",
    "    \n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# paramètre Lambda en la mettant dans \n",
    "# la liste de la variable range_lambda.\n",
    "# Specify the search range for the \n",
    "# parameter Lambda by putting it in \n",
    "# the list of the variable range_lambda.\n",
    "range_lambda = [10**n for n in range(-5, 6)]\n",
    "# ******\n",
    "\n",
    "results['Range_lambda'].append(range_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# paramètre Sigma en la mettant dans \n",
    "# la liste de la variable range_sigma.\n",
    "# Specify the search range for the \n",
    "# parameter Sigma by putting it in \n",
    "# the list of the variable range_sigma.\n",
    "range_sigma = [round(0.1*n, 1) for n in range(1, 10)]\n",
    "# ******\n",
    "\n",
    "results['Range_sigma'].append(range_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Optimisez ici les paramètres lambda et sigma de votre \n",
    "# classifieur en effectuant une recherche en grille.\n",
    "# Optimize here the lambda and sigma parameters of your \n",
    "# classifier by performing a grid search.\n",
    "# ******\n",
    "lambda_o, sigma_o = 0, 0\n",
    "s = -1\n",
    "\n",
    "for lambda_ in range_lambda:\n",
    "    for sigma in range_sigma:\n",
    "#         print(\"lambda=\", lambda_, \"sigma=\", sigma)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=69)\n",
    "        clf = DiscriminantANoyau(lambda_, sigma, verbose=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        tmp = clf.score(X_test, y_test)\n",
    "        if tmp > s:\n",
    "            lambda_o, sigma_o = lambda_, sigma\n",
    "            s = tmp\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# paramètre Lambda en la mettant dans la\n",
    "# variable best_lambda en remplaçant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Lambda by putting it in the\n",
    "# variable best_lambda by replacing the 0.\n",
    "best_lambda = lambda_o\n",
    "# ******\n",
    "\n",
    "results['Best_lambda'].append(best_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# paramètre Sigma en la mettant dans la\n",
    "# variable best_sigma en remplaçant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Sigma by putting it in the\n",
    "# variable best_sigma by replacing the 0.\n",
    "best_sigma = sigma_o\n",
    "# ******\n",
    "\n",
    "results['Best_sigma'].append(best_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Une fois les paramètres lambda et\n",
    "# sigma de votre classifieur optimisés,\n",
    "# créez une instance de ce classifieur\n",
    "# en utilisant ces paramètres optimaux.\n",
    "# Once the lambda and sigma parameters of your classifier are optimized,\n",
    "# create an instance of this classifier\n",
    "# using these optimal parameters.\n",
    "# ******\n",
    "clf_o = DiscriminantANoyau(best_lambda, best_sigma, verbose=False)\n",
    "clf_o.fit(X1, y1)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indicate the error rate obtained on the training set in the variable\n",
    "# err_train variable by replacing the 0.\n",
    "err_train = 1 - clf_o.score(X1, y1)\n",
    "# ******\n",
    "\n",
    "results['Error_train'].append(err_train)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez le taux d'erreur obtenu sur le\n",
    "# jeu de test dans la variable \n",
    "# err_test en remplaçant le 0.\n",
    "# Indicate the error rate obtained on the\n",
    "# test set in the err_test variable by replacing the 0.\n",
    "err_test = 1 - clf_o.score(X2, y2)\n",
    "# ******\n",
    "\n",
    "results['Error_test'].append(err_test)\n",
    "\n",
    "# *** TODO ***\n",
    "# Créez ici une grille permettant d'afficher les régions de décision pour chaque classifieur\n",
    "# Indice : numpy.meshgrid pourrait vous être utile ici\n",
    "# Par la suite, affichez les régions de décision dans la même figure que les données de test.\n",
    "# Note : un pas de 0.02 pour le meshgrid est recommandé\n",
    "# ******\n",
    "h = .02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, h), numpy.arange(y_min, y_max, h))\n",
    "\n",
    "pyplot.figure()\n",
    "\n",
    "colors = numpy.array([x for x in \"bgrcmyk\"])\n",
    "\n",
    "y_pred = clf_o.predict(numpy.c_[xx.ravel(), yy.ravel()]) \n",
    "y_pred = y_pred.reshape(xx.shape)\n",
    "        \n",
    "pyplot.contourf(xx, yy, y_pred, cmap=pyplot.cm.Paired, alpha=0.5)\n",
    "pyplot.scatter(X[:, 0], X[:, 1], cmap=pyplot.cm.Paired, c=colors[y])\n",
    "        \n",
    "pyplot.xlim(xx.min(), xx.max())\n",
    "pyplot.ylim(yy.min(), yy.max())\n",
    "\n",
    "# On affiche la figure\n",
    "_times.append(time.time())\n",
    "checkTime(TMAX_EVAL, \"Evaluation\")\n",
    "pyplot.show()\n",
    "\n",
    "# Affichage des résultats\n",
    "df = pandas.DataFrame(results)\n",
    "display.display(df)"
   ]
  }
 ],
 "metadata": {
  "PAX": {
   "userLang": "fr"
  },
  "celltoolbar": "",
  "jupytext": {
   "notebook_metadata_filter": "celltoolbar",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (PAX)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fr",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
