{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03d44a27",
   "metadata": {
    "editable": false,
    "id": "49603f07",
    "lang": "fr",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Devoir 3, Question 3 : Discriminant avec noyau et descente du gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a17b1a",
   "metadata": {
    "editable": false,
    "id": "e36ddca2",
    "lang": "en",
    "tags": [
     "problem-title"
    ]
   },
   "source": [
    "# Homework 3, Question 3: Kernel discriminant and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55eb7d03",
   "metadata": {
    "editable": false,
    "id": "a728e2e6",
    "lang": "fr",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Code pr√©ambule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a661e",
   "metadata": {
    "editable": false,
    "id": "1e202079",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "## Preamble code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f56a2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-18T04:29:02.317763Z",
     "start_time": "2021-12-18T04:29:01.342576Z"
    },
    "editable": false,
    "id": "508a9904",
    "tags": [
     "problem-context",
     "autoexec"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy\n",
    "import itertools\n",
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', 0)\n",
    "import collections\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "# Nous ne voulons pas √™tre signal√© par ce type d'avertissement, non pertinent pour le devoir\n",
    "# We don't want to be signaled of this warning, irrelevant for the homework\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# Fonction pour v√©rifier le temps d'ex√©cution\n",
    "# Function to verify execution time\n",
    "_times = []\n",
    "def checkTime(maxduration, question):\n",
    "    duration = _times[-1] - _times[-2]\n",
    "    if duration > maxduration:\n",
    "        print(\"[ATTENTION] Votre code pour la question {0} met trop de temps √† s'ex√©cuter! \".format(question)+\n",
    "              \"Le temps maximum permis est de {0:.4f} secondes, \".format(maxduration)+\n",
    "              \"mais votre code a requis {0:.4f} secondes! \".format(duration)+\n",
    "              \"Assurez-vous que vous ne faites pas d'appels bloquants (par exemple √† show()) dans cette boucle!\")\n",
    "\n",
    "# D√©finition des dur√©es d'ex√©cution maximales pour chaque classifieur\n",
    "# Definition of maximum execution time for each classifier\n",
    "TMAX_EVAL = 3.0\n",
    "TMAX_FIT = 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e610261d",
   "metadata": {
    "editable": false,
    "id": "80706621",
    "lang": "fr",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "Soit un discriminant avec noyau gaussien, entra√Æn√© par descente du gradient avec la fonction\n",
    "d‚Äôerreur suivante :\n",
    "$$E(\\mathbf{\\alpha},w_0|\\mathcal{X})=\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t},$$\n",
    "avec :\n",
    "$$\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})=\\sum_{\\mathbf{x}^{s}\\in\\mathcal{X}}{\\alpha^{s}r^{s}}K(\\mathbf{x}^{s},\\mathbf{x}^{t})+w_{0},$$\n",
    "$$\\mathcal{Y}=\\{\\mathbf{x}^{t}\\in\\mathcal{X}\\,|\\,r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})<1\\},$$\n",
    "$$\\alpha^{t}\\geq 0,~\\forall t.$$\n",
    "\n",
    "- Les valeurs cibles sont dans $r^{t}\\in\\{‚àí1,1\\}$;\n",
    "- Le classement se fait selon le signe de $\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})$;\n",
    "- Le param√®tre $\\lambda$ permet de contr√¥ler le niveau de r√©gularisation effectu√©."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5bf47e",
   "metadata": {
    "editable": false,
    "id": "60090dc1",
    "lang": "en",
    "tags": [
     "problem-statement"
    ]
   },
   "source": [
    "Consider a discriminant with a Gaussian kernel, trained by gradient descent with the following error function:\n",
    "$$E(\\mathbf{\\alpha},w_0|\\mathcal{X})=\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t},$$\n",
    "with:\n",
    "$$\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})=\\sum_{\\mathbf{x}^{s}\\in\\mathcal{X}}{\\alpha^{s}r^{s}}K(\\mathbf{x}^{s},\\mathbf{x}^{t})+w_{0},$$\n",
    "$$\\mathcal{Y}=\\{\\mathbf{x}^{t}\\in\\mathcal{X}\\,|\\,r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})<1\\},$$\n",
    "$$\\alpha^{t}\\geq 0,~\\forall t.$$\n",
    "\n",
    "- The target values are in $r^{t}\\in\\{‚àí1,1\\}$;\n",
    "- The classification is done according to the sign of $\\mathrm{h}(\\mathbf{x}^{t}|\\mathbf{\\alpha},w_{0})$;\n",
    "- The $\\lambda$ parameter is used to control the level of regularization performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0920f0",
   "metadata": {
    "editable": false,
    "id": "59129adf",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "## Q3A\n",
    "D√©veloppez les √©quations des gradients des poids $\\alpha^{t}$ et de la constante $w_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78edb484",
   "metadata": {
    "editable": false,
    "id": "5a30d679",
    "lang": "en",
    "tags": []
   },
   "source": [
    "## Q3A\n",
    "Develop the equations of the gradients of the weights $\\alpha^{t}$ and the constant $w_{0}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d245d15",
   "metadata": {
    "editable": false,
    "id": "c794f780",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Entrez votre solution √† Q3A dans la cellule ci-dessous (markdown et $\\LaTeX$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2ccf6",
   "metadata": {
    "editable": false,
    "id": "92c46989",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Enter your answer to Q3A in the cell below (markdown and $\\LaTeX$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807c457f",
   "metadata": {
    "editable": false,
    "tags": [
     "feedback"
    ]
   },
   "source": [
    "\n",
    "        <div class=\"feedback-cell\" style=\"background: rgba(100 , 100 , 100 , 0.4)\">\n",
    "            <h3>Votre soumission a √©t√© enregistr√©e!</h3>\n",
    "            <ul>\n",
    "                <li>notez qu'il n'y a <strong>pas</strong> de correction automatique pour cet exercice&puncsp;;</li>\n",
    "                <li>par cons√©quent, votre note est <strong>actuellement</strong> z√©ro&puncsp;;</li>\n",
    "                <li>elle sera cependant ajust√©e par le professeur d√®s que la correction manuelle sera compl√©t√©e&puncsp;;</li>\n",
    "                <li>vous pouvez soumettre autant de fois que n√©cessaire jusqu'√† la date d'√©ch√©ance&puncsp;;</li>\n",
    "                <li>mais √©vitez de soumettre inutilement.</li>\n",
    "            </ul>\n",
    "        </div>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316682d3",
   "metadata": {
    "deletable": false,
    "id": "1b67c907",
    "tags": [
     "user-answer-D3Q3A",
     "editable"
    ]
   },
   "source": [
    "$\\begin{align*} \n",
    "E(\\mathbf{\\alpha},w_0|\\mathcal{X}) \n",
    "&= \\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t}\n",
    "\\end{align*}$\n",
    "\n",
    "Descente du gradient:\\\n",
    "$\\begin{align*} \n",
    "\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial\\mathbf{\\alpha}^*}\n",
    "&= \\frac{\\partial}{\\partial\\mathbf{\\alpha}^*} \\bigg(\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\left[1-r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})\\right]+\\lambda\\sum_{t=1}^N\\alpha^{t}\\bigg) \\\\\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\frac{\\partial r^{t}\\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})}{\\partial\\mathbf{\\alpha}^*} +  \\lambda\\frac{\\partial\\sum_{t=1}^N\\alpha^{t}}{\\partial\\mathbf{\\alpha}^*} \\\\\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}} \\frac{\\partial r^{t}\\mathrm{h}}{\\partial\\mathrm{h}}\\frac{\\partial \\mathrm{h}(\\mathbf{x}^{t}|\\alpha,w_{0})}{\\partial\\mathbf{\\alpha}^*} +  \\lambda\\frac{\\partial\\sum_{t=1}^N\\alpha^{t}}{\\partial\\mathbf{\\alpha}^*} \\\\\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}r^{*}K(\\mathbf{x}^{*},\\mathbf{x}^{t})  +  \\lambda\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*} \n",
    "\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial w_0}\n",
    "&= -\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}\n",
    "\\end{align*}$\n",
    "\n",
    "On a donc: \\\n",
    "$\\mathbf{\\Delta}\\mathbf{\\alpha}^t \n",
    "= -\\eta\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial\\mathbf{\\alpha}^t}\n",
    "= \\eta\\big(\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}r^{*}K(\\mathbf{x}^{*},\\mathbf{x}^{t})  -  \\lambda\\big)$\n",
    "\n",
    "$\\mathbf{\\Delta}w_0 = \n",
    "= -\\eta\\frac{\\partial E(\\mathbf{\\alpha},w_0|\\mathcal{X})}{\\partial w_0}\n",
    "= \\eta\\sum_{\\mathbf{x}^{t}\\in\\mathcal{Y}}r^{t}$\n",
    "\n",
    "Mise √† jour des poids $\\mathbf{\\alpha}^t$ et $w_0$: \\\n",
    "$\\mathbf{\\alpha}^t = \n",
    "\\begin{cases}\n",
    "0 & si & \\mathbf{\\alpha}^t + \\mathbf{\\Delta}\\mathbf{\\alpha}^t < 0 \\\\\n",
    "\\mathbf{\\alpha}^t + \\mathbf{\\Delta}\\mathbf{\\alpha}^t & autrement\n",
    "\\end{cases}$\n",
    "\n",
    "$w_0 = w_0 + \\mathbf{\\Delta}w_0$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c32ce24",
   "metadata": {
    "editable": false,
    "id": "80da7504",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "## Q3B\n",
    "Impl√©mentez ce classifieur en respectant l‚Äôinterface scikit-learn, en donnant au minimum les fonctions `fit`, `predict` et `score`. Pour l‚Äôoptimisation des param√®tres du classifieur, utilisez la m√©thode L-BFGS disponible dans SciPy (`scipy.optimize.fmin_l_bfgs_b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e12461",
   "metadata": {
    "editable": false,
    "id": "b6175747",
    "lang": "en",
    "tags": []
   },
   "source": [
    "## Q3B\n",
    "Implement this classifier while respecting the scikit-learn interface, giving at least the functions `fit`, `predict` and `score`. For the optimization of the classifier parameters, use the L-BFGS method available in SciPy (`scipy.optimize.fmin_l_bfgs_b`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223cd105",
   "metadata": {
    "editable": false,
    "id": "f48c8818",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code r√©ponse √† l'exercice Q3B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e0564d",
   "metadata": {
    "editable": false,
    "id": "49a088e9",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3B answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6c2ad3",
   "metadata": {
    "editable": false,
    "id": "d9402224",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Impl√©mentation du discriminant √† noyau\n",
    "# Kernel discriminant implementation\n",
    "class DiscriminantANoyau:\n",
    "    \n",
    "    def __init__(self, lambda_, sigma, verbose=True):\n",
    "        # Cette fonction est d√©j√† cod√©e pour vous, vous n'avez qu'√† utiliser les variables membres qu'elle \n",
    "        # d√©finit dans les autres fonctions de cette classe. Lambda et sigma sont d√©finis dans l'√©nonc√©.\n",
    "        # verbose permet d'afficher certaines statistiquesen lien avec la convergence de fmin_l_bfgs_b.\n",
    "        # This function is already coded for you, you just have to use the member variables it defines in \n",
    "        # in the other functions of this class. Lambda and sigma are defined in the statement.\n",
    "        # verbose allows you to display some statistics related to the convergence of the fmin_l_bfgs_b.\n",
    "        self.lambda_ = lambda_\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Impl√©mentez la fonction d'entra√Ænement du classifieur, selon les √©quations d√©velopp√©es √† Q3A\n",
    "        # Implement the training function of the classifier, according to the equations developed in Q3A\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Vous devez √©crire une fonction nomm√©e evaluateFunc, qui re√ßoit un seul argument, soit les valeurs\n",
    "        # des param√®tres pour lesquels on souhaite conna√Ætre l'erreur et le gradient. Cette fonction sera \n",
    "        # appel√©e √† r√©p√©tition par l'optimiseur de scipy, qui l'utilisera pour minimiser l'erreur et \n",
    "        # obtenir un jeu de param√®tres optimal.\n",
    "        # You must write a function named evaluateFunc, which receives only one argument, either the values\n",
    "        # of the parameters for which you want to know the error and the gradient. This function will be \n",
    "        # called repeatedly by the scipy optimizer, which will use it to minimize the error and \n",
    "        # to obtain an optimal set of parameters.\n",
    "        def evaluateFunc(params):\n",
    "            # Ecrire ici le code de la fonction evaluateFunc calculant l'erreur \n",
    "            # et le gradient de params.\n",
    "            # Write here the code of the evaluateFunc function calculating the error \n",
    "            # and the gradient of params.\n",
    "            return err, grad\n",
    "        # ******\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Initialisez al√©atoirement les param√®tres alpha^t et w_0 (l'optimiseur requiert une valeur initiale, \n",
    "        # et nous ne pouvons pas simplement n'utiliser que des z√©ros pour diff√©rentes raisons). Stockez ces \n",
    "        # valeurs initiales al√©atoires dans un array numpy nomm√© \"params\". D√©terminez √©galement les bornes √†\n",
    "        # utiliser sur ces param√®tres et stockez les dans une variable nomm√©e \"bounds\".\n",
    "        # Indice : les param√®tres peuvent-ils avoir une valeur maximale (au-dessus de laquelle ils ne veulent\n",
    "        # plus rien dire)? Une valeur minimale? R√©f√©rez-vous √† la documentation de fmin_l_bfgs_b pour savoir\n",
    "        # comment indiquer l'absence de bornes.\n",
    "        # Randomly initialize the parameters alpha^t and w_0 (the optimizer requires an initial value, \n",
    "        # and we can't just use zeros for various reasons). Store these \n",
    "        # random initial values in a numpy array named \"params\". Also determine the bounds to use on these\n",
    "        # parameters and store them in a variable named \"bounds\".\n",
    "        # Hint: Can the parameters have a maximum value (above which they mean nothing)?\n",
    "        # mean anything)? A minimum value? Refer to the documentation of fmin_l_bfgs_b to know\n",
    "        # how to indicate the absence of bounds.\n",
    "        # ******\n",
    "\n",
    "        # √Ä ce stade, trois choses devraient √™tre d√©finies :\n",
    "        # - Une fonction d'√©valuation nomm√©e evaluateFunc, capable de retourner l'erreur et le gradient d'erreur \n",
    "        #   pour chaque param√®tre, pour une configuration d'alpha et w_0 donn√©e.\n",
    "        # - Un tableau numpy nomm√© params de m√™me taille que le nombre de param√®tres √† entra√Æner.\n",
    "        # - Une liste nomm√©e bounds contenant les bornes que l'optimiseur doit respecter pour chaque param√®tre.\n",
    "        # On appelle maintenant l'optimiseur avec ces informations et on conserve les valeurs dans params\n",
    "        # At this point, three things should be defined:\n",
    "        # - An evaluation function named evaluateFunc, capable of returning the error and error gradient \n",
    "        # for each parameter, for a given alpha and w_0 configuration.\n",
    "        # - A numpy array named params of the same size as the number of parameters to train.\n",
    "        # - A list named bounds containing the bounds that the optimizer must respect for each parameter.\n",
    "        # We now call the optimizer with this information and we keep the values in params\n",
    "        _times.append(time.time())\n",
    "        params, minval, infos = fmin_l_bfgs_b(evaluateFunc, params, bounds=bounds)\n",
    "        _times.append(time.time())\n",
    "        checkTime(TMAX_FIT, \"Entrainement\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            # On affiche quelques statistiques / display some statistics\n",
    "            print(\"Entra√Ænement termin√© apr√®s {it} it√©rations et {calls} appels √† evaluateFunc\".format(it=infos['nit'], calls=infos['funcalls']))\n",
    "            print(\"\\tErreur minimale : {:.5f}\".format(minval))\n",
    "            print(\"\\tL'algorithme a converg√©\" if infos['warnflag'] == 0 else \"\\tL'algorithme n'a PAS converg√©\")\n",
    "            print(\"\\tGradients des param√®tres √† la convergence (ou √† l'√©puisement des ressources) :\")\n",
    "            print(infos['grad'])\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Stockez les param√®tres optimis√©s de la fa√ßon suivante :\n",
    "        # - Le vecteur alpha dans self.alphas;\n",
    "        # - Le biais w_0 dans self.w0.\n",
    "        # Store the optimized parameters as follows:\n",
    "        # - The alpha vector in self.alphas;\n",
    "        # - The bias w_0 in self.w0.\n",
    "        # ******\n",
    "\n",
    "        # On retient √©galement le jeu d'entra√Ænement, qui pourra vous √™tre utile pour les autres fonctions.\n",
    "        # We also retain the training set, which can be useful for other functions.\n",
    "        self.X, self.y = X, y\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Impl√©mentez la fonction d'inf√©rence (pr√©diction). Vous pouvez supposer que fit() a pr√©alablement √©t√©\n",
    "        # ex√©cut√© et que les variables membres alphas, w0, X et y existent. N'oubliez pas que ce classifieur doit\n",
    "        # retourner -1 ou 1\n",
    "        # Implement the inference (prediction) function. You can assume that fit() has been previously executed and that the\n",
    "        # executed and that the member variables alphas, w0, X and y exist. Remember that this classifier must\n",
    "        # return -1 or 1\n",
    "        # ******\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Impl√©mentez la fonction retournant le score (accuracy) du classifieur sur les donn√©es re√ßues en\n",
    "        # argument. Vous pouvez supposer que fit() a pr√©alablement √©t√© ex√©cut√©e\n",
    "        # Indice : r√©utiliser votre impl√©mentation de predict() r√©duit de beaucoup la taille de cette fonction!\n",
    "        # Implement the function returning the accuracy of the classifier on the data received as\n",
    "        # argument. You can assume that fit() has already been executed\n",
    "        # Hint: reusing your implementation of predict() reduces the size of this function by a lot!\n",
    "        # ******\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87edb73",
   "metadata": {
    "editable": false,
    "id": "0381df3e",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Entrez votre solution √† Q3B dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294eeca9",
   "metadata": {
    "editable": false,
    "id": "6238b53f",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Enter your answer to Q3B in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce725f",
   "metadata": {
    "deletable": false,
    "id": "1ec4df02",
    "tags": [
     "user-answer-D3Q3B",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Impl√©mentation du discriminant √† noyau\n",
    "# Kernel discriminant implementation\n",
    "class DiscriminantANoyau:\n",
    "    \n",
    "    def __init__(self, lambda_, sigma, verbose=True):\n",
    "        # Cette fonction est d√©j√† cod√©e pour vous, vous n'avez qu'√† utiliser les variables membres qu'elle \n",
    "        # d√©finit dans les autres fonctions de cette classe. Lambda et sigma sont d√©finis dans l'√©nonc√©.\n",
    "        # verbose permet d'afficher certaines statistiquesen lien avec la convergence de fmin_l_bfgs_b.\n",
    "        # This function is already coded for you, you just have to use the member variables it defines in \n",
    "        # in the other functions of this class. Lambda and sigma are defined in the statement.\n",
    "        # verbose allows you to display some statistics related to the convergence of the fmin_l_bfgs_b.\n",
    "        self.lambda_ = lambda_\n",
    "        self.sigma = sigma\n",
    "        self.verbose = verbose \n",
    "        \n",
    "    def kernel(self, a, b):\n",
    "        return numpy.exp(-cdist(a, b)**2 / self.sigma**2)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # Impl√©mentez la fonction d'entra√Ænement du classifieur, selon les √©quations d√©velopp√©es √† Q3A\n",
    "        # Implement the training function of the classifier, according to the equations developed in Q3A\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Vous devez √©crire une fonction nomm√©e evaluateFunc, qui re√ßoit un seul argument, soit les valeurs\n",
    "        # des param√®tres pour lesquels on souhaite conna√Ætre l'erreur et le gradient. Cette fonction sera \n",
    "        # appel√©e √† r√©p√©tition par l'optimiseur de scipy, qui l'utilisera pour minimiser l'erreur et \n",
    "        # obtenir un jeu de param√®tres optimal.\n",
    "        # You must write a function named evaluateFunc, which receives only one argument, either the values\n",
    "        # of the parameters for which you want to know the error and the gradient. This function will be \n",
    "        # called repeatedly by the scipy optimizer, which will use it to minimize the error and \n",
    "        # to obtain an optimal set of parameters.\n",
    "        def evaluateFunc(params):\n",
    "            # Ecrire ici le code de la fonction evaluateFunc calculant l'erreur \n",
    "            # et le gradient de params.\n",
    "            # Write here the code of the evaluateFunc function calculating the error \n",
    "            # and the gradient of params.\n",
    "            \n",
    "            w0 = params[0]\n",
    "            alphas = params[1:]\n",
    "            \n",
    "            hypothesis = numpy.sum(self.kernel(X, X) * y *  alphas, axis=1) + w0\n",
    "            f = numpy.where(y.reshape(-1) * hypothesis.reshape(-1) < 1, True, False).reshape(-1)\n",
    "\n",
    "            err = numpy.sum(1 - y[f] * hypothesis[f]) + self.lambda_ * numpy.sum(alphas)\n",
    "            \n",
    "            delta_alphas =  y * numpy.sum(y[f] * self.kernel(X, X[f, :])) - self.lambda_\n",
    "            delta_w0 = numpy.sum(y[f])\n",
    "            grad = numpy.concatenate(([delta_w0], delta_alphas.reshape(-1)))\n",
    "            \n",
    "            return err, grad\n",
    "        # ******\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Initialisez al√©atoirement les param√®tres alpha^t et w_0 (l'optimiseur requiert une valeur initiale, \n",
    "        # et nous ne pouvons pas simplement n'utiliser que des z√©ros pour diff√©rentes raisons). Stockez ces \n",
    "        # valeurs initiales al√©atoires dans un array numpy nomm√© \"params\". D√©terminez √©galement les bornes √†\n",
    "        # utiliser sur ces param√®tres et stockez les dans une variable nomm√©e \"bounds\".\n",
    "        # Indice : les param√®tres peuvent-ils avoir une valeur maximale (au-dessus de laquelle ils ne veulent\n",
    "        # plus rien dire)? Une valeur minimale? R√©f√©rez-vous √† la documentation de fmin_l_bfgs_b pour savoir\n",
    "        # comment indiquer l'absence de bornes.\n",
    "        # Randomly initialize the parameters alpha^t and w_0 (the optimizer requires an initial value, \n",
    "        # and we can't just use zeros for various reasons). Store these \n",
    "        # random initial values in a numpy array named \"params\". Also determine the bounds to use on these\n",
    "        # parameters and store them in a variable named \"bounds\".\n",
    "        # Hint: Can the parameters have a maximum value (above which they mean nothing)?\n",
    "        # mean anything)? A minimum value? Refer to the documentation of fmin_l_bfgs_b to know\n",
    "        # how to indicate the absence of bounds.\n",
    "        # ******\n",
    "        params = numpy.random.rand(X.shape[0] + 1)\n",
    "        params = params.reshape((params.shape[0], 1)) \n",
    "        \n",
    "        bounds = numpy.array([(0, None)]*X.shape[0] + [(None, None)])\n",
    "\n",
    "        # √Ä ce stade, trois choses devraient √™tre d√©finies :\n",
    "        # - Une fonction d'√©valuation nomm√©e evaluateFunc, capable de retourner l'erreur et le gradient d'erreur \n",
    "        #   pour chaque param√®tre, pour une configuration d'alpha et w_0 donn√©e.\n",
    "        # - Un tableau numpy nomm√© params de m√™me taille que le nombre de param√®tres √† entra√Æner.\n",
    "        # - Une liste nomm√©e bounds contenant les bornes que l'optimiseur doit respecter pour chaque param√®tre.\n",
    "        # On appelle maintenant l'optimiseur avec ces informations et on conserve les valeurs dans params\n",
    "        # At this point, three things should be defined:\n",
    "        # - An evaluation function named evaluateFunc, capable of returning the error and error gradient \n",
    "        # for each parameter, for a given alpha and w_0 configuration.\n",
    "        # - A numpy array named params of the same size as the number of parameters to train.\n",
    "        # - A list named bounds containing the bounds that the optimizer must respect for each parameter.\n",
    "        # We now call the optimizer with this information and we keep the values in params\n",
    "        _times.append(time.time())\n",
    "        params, minval, infos = fmin_l_bfgs_b(evaluateFunc, params, bounds=bounds)\n",
    "        _times.append(time.time())\n",
    "        checkTime(TMAX_FIT, \"Entrainement\")\n",
    "        \n",
    "        if self.verbose:\n",
    "            # On affiche quelques statistiques / display some statistics\n",
    "            print(\"Entra√Ænement termin√© apr√®s {it} it√©rations et {calls} appels √† evaluateFunc\".format(it=infos['nit'], calls=infos['funcalls']))\n",
    "            print(\"\\tErreur minimale : {:.5f}\".format(minval))\n",
    "            print(\"\\tL'algorithme a converg√©\" if infos['warnflag'] == 0 else \"\\tL'algorithme n'a PAS converg√©\")\n",
    "            print(\"\\tGradients des param√®tres √† la convergence (ou √† l'√©puisement des ressources) :\")\n",
    "            print(infos['grad'])\n",
    "\n",
    "        # *** TODO ***\n",
    "        # Stockez les param√®tres optimis√©s de la fa√ßon suivante :\n",
    "        # - Le vecteur alpha dans self.alphas;\n",
    "        # - Le biais w_0 dans self.w0.\n",
    "        # Store the optimized parameters as follows:\n",
    "        # - The alpha vector in self.alphas;\n",
    "        # - The bias w_0 in self.w0.\n",
    "        # ******\n",
    "        self.alphas = params[1:]\n",
    "        self.w0 = params[0]\n",
    "\n",
    "        # On retient √©galement le jeu d'entra√Ænement, qui pourra vous √™tre utile pour les autres fonctions.\n",
    "        # We also retain the training set, which can be useful for other functions.\n",
    "        self.X, self.y = X, y\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Impl√©mentez la fonction d'inf√©rence (pr√©diction). Vous pouvez supposer que fit() a pr√©alablement √©t√©\n",
    "        # ex√©cut√© et que les variables membres alphas, w0, X et y existent. N'oubliez pas que ce classifieur doit\n",
    "        # retourner -1 ou 1\n",
    "        # Implement the inference (prediction) function. You can assume that fit() has been previously executed and that the\n",
    "        # executed and that the member variables alphas, w0, X and y exist. Remember that this classifier must\n",
    "        # return -1 or 1\n",
    "        # ******\n",
    "        hypothesis = numpy.sum(self.kernel(X, self.X) * self.y * self.alphas.reshape(-1), axis=1) + self.w0\n",
    "        predictions = numpy.where(hypothesis >= 0, 1, -1)\n",
    "        return predictions\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \n",
    "        # *** TODO ***\n",
    "        # Impl√©mentez la fonction retournant le score (accuracy) du classifieur sur les donn√©es re√ßues en\n",
    "        # argument. Vous pouvez supposer que fit() a pr√©alablement √©t√© ex√©cut√©e\n",
    "        # Indice : r√©utiliser votre impl√©mentation de predict() r√©duit de beaucoup la taille de cette fonction!\n",
    "        # Implement the function returning the accuracy of the classifier on the data received as\n",
    "        # argument. You can assume that fit() has already been executed\n",
    "        # Hint: reusing your implementation of predict() reduces the size of this function by a lot!\n",
    "        # ******\n",
    "        score = numpy.sum(numpy.where(y * self.predict(X) >= 0, 1, 0)) / X.shape[0]\n",
    "        return score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482aa754",
   "metadata": {
    "editable": false,
    "id": "96e3d292",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "## Q3C\n",
    "Exp√©rimentez avec le classifieur d√©velopp√© √† la question pr√©c√©dente (Q3B) en utilisant un jeu de 800 donn√©es synth√©tiques *moons* selon deux classes et avec du bruit blanc, soit $\\sigma_{bruit}=0.3$ avec $datasets.make\\_moons(n\\_samples=800, noise=0.3)$.\n",
    "\n",
    "Divisez le jeu de donn√©es en deux parties de taille √©gale, une pour l‚Äôentra√Ænement et une pour le test. Affichez les donn√©es ainsi que la fronti√®re obtenue avec le classifieur pour une param√©trisation de $\\lambda$ et $\\sigma$ (√©talement du noyau gaussien) permettant d‚Äôobtenir un taux d‚Äôerreur inf√©rieur √† 10 % en √©valuation sur le jeu de test. Pour obtenir un taux d‚Äôerreur inf√©rieur √† 10 % sur le jeu de donn√©es moons, il vous faudra probablement resserrer la recherche en grille sur le param√®tre $\\sigma\\in\\left[0,1\\right]$.\n",
    "\n",
    "Pour cette question, donnez :\n",
    "- Les diff√©rents param√®tres d‚Äôentra√Ænement √©valu√©s par la recherche en grille ainsi que les performances associ√©es;\n",
    "- Les valeurs de param√®tres finaux retenus ainsi que les performances correspondantes (entra√Ænement et test);\n",
    "- Le graphique des fronti√®res de d√©cision de la configuration retenue et des donn√©es utilis√©es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7e4915",
   "metadata": {
    "editable": false,
    "id": "a96a21ee",
    "lang": "en",
    "tags": []
   },
   "source": [
    "## Q3C\n",
    "Experiment with the classifier developed at the previous question (Q3B) using a set of 800 synthetic data *moons* according to two classes and with white noise, that is $\\sigma_{bruit}=0.3$ with $datasets.make\\_moons(n\\_samples=800, noise=0.3)$.\n",
    "\n",
    "Divide the dataset into two equally sized parts, one for training and one for testing. Display the data as well as the frontier obtained with the classifier for a parameterization of $\\lambda$ and $\\sigma$ (Gaussian kernel spreading) allowing to obtain an error rate of less than 10% in evaluation on the test set. To get an error rate of less than 10% on the moons dataset, you will probably need to tighten the grid search on the $\\sigma\\in\\left[0,1\\right]$ parameter.\n",
    "\n",
    "For this question, give:\n",
    "- The various training parameters evaluated by grid research as well as the associated performances;\n",
    "- The final parameter values retained as well as the corresponding performances (training and test);\n",
    "- The graph of the decision boundaries of the configuration selected and the data used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e924f079",
   "metadata": {
    "editable": false,
    "id": "09a66670",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Patron de code r√©ponse √† l'exercice Q3C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fa40ad",
   "metadata": {
    "editable": false,
    "id": "5de98b15",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Q3C answer code template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852a2a6d",
   "metadata": {
    "editable": false,
    "id": "be19f805",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cr√©ation du tableau pour accumuler les r√©sultats\n",
    "# Creation of the table to accumulate the results\n",
    "results = {'Classifier':[\"DiscriminantANoyau\"],\n",
    "           'Range_lambda':[],\n",
    "           'Range_sigma':[],\n",
    "           'Best_lambda':[],\n",
    "           'Best_sigma':[],\n",
    "           'Error_train':[],\n",
    "           'Error_test':[],\n",
    "          }\n",
    "\n",
    "# *** TODO ***\n",
    "# Cr√©ez le jeu de donn√©es √† partir de la fonction make_moons, tel que demand√© dans l'√©nonc√©\n",
    "# N'oubliez pas de vous assurer que les valeurs possibles de y sont bel et bien dans -1 et 1, et non 0 et 1!\n",
    "# Create the dataset from the make_moons function, as requested in the statement\n",
    "# Remember to make sure that the possible values of y are in -1 and 1, not 0 and 1!\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# S√©parez le jeu de donn√©es en deux parts √©gales, l'une pour l'entra√Ænement et l'autre pour le test.\n",
    "# Separate the dataset into two equal parts, one for training and one for testing.\n",
    "# ******\n",
    "\n",
    "_times.append(time.time())\n",
    "    \n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# param√®tre Lambda en la mettant dans \n",
    "# la liste de la variable range_lambda.\n",
    "# Specify the search range for the \n",
    "# parameter Lambda by putting it in \n",
    "# the list of the variable range_lambda.\n",
    "range_lambda = []\n",
    "# ******\n",
    "\n",
    "results['Range_lambda'].append(range_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# param√®tre Sigma en la mettant dans \n",
    "# la liste de la variable range_sigma.\n",
    "# Specify the search range for the \n",
    "# parameter Sigma by putting it in \n",
    "# the list of the variable range_sigma.\n",
    "range_sigma = []\n",
    "# ******\n",
    "\n",
    "results['Range_sigma'].append(range_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Optimisez ici les param√®tres lambda et sigma de votre \n",
    "# classifieur en effectuant une recherche en grille.\n",
    "# Optimize here the lambda and sigma parameters of your \n",
    "# classifier by performing a grid search.\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# param√®tre Lambda en la mettant dans la\n",
    "# variable best_lambda en rempla√ßant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Lambda by putting it in the\n",
    "# variable best_lambda by replacing the 0.\n",
    "best_lambda = 0\n",
    "# ******\n",
    "\n",
    "results['Best_lambda'].append(best_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# param√®tre Sigma en la mettant dans la\n",
    "# variable best_sigma en rempla√ßant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Sigma by putting it in the\n",
    "# variable best_sigma by replacing the 0.\n",
    "best_sigma = 0\n",
    "# ******\n",
    "\n",
    "results['Best_sigma'].append(best_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Une fois les param√®tres lambda et\n",
    "# sigma de votre classifieur optimis√©s,\n",
    "# cr√©ez une instance de ce classifieur\n",
    "# en utilisant ces param√®tres optimaux.\n",
    "# Once the lambda and sigma parameters of your classifier are optimized,\n",
    "# create an instance of this classifier\n",
    "# using these optimal parameters.\n",
    "# ******\n",
    "\n",
    "# *** TODO ***\n",
    "# Indicate the error rate obtained on the training set in the variable\n",
    "# err_train variable by replacing the 0.\n",
    "err_train = 0\n",
    "# ******\n",
    "\n",
    "results['Error_train'].append(err_train)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez le taux d'erreur obtenu sur le\n",
    "# jeu de test dans la variable \n",
    "# err_test en rempla√ßant le 0.\n",
    "# Indicate the error rate obtained on the\n",
    "# test set in the err_test variable by replacing the 0.\n",
    "err_test = 0\n",
    "# ******\n",
    "\n",
    "results['Error_test'].append(err_test)\n",
    "\n",
    "# *** TODO ***\n",
    "# Cr√©ez ici une grille permettant d'afficher les r√©gions de d√©cision pour chaque classifieur\n",
    "# Indice : numpy.meshgrid pourrait vous √™tre utile ici\n",
    "# Par la suite, affichez les r√©gions de d√©cision dans la m√™me figure que les donn√©es de test.\n",
    "# Note : un pas de 0.02 pour le meshgrid est recommand√©\n",
    "# ******\n",
    "\n",
    "# On affiche la figure\n",
    "_times.append(time.time())\n",
    "checkTime(TMAX_EVAL, \"Evaluation\")\n",
    "pyplot.show()\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "df = pandas.DataFrame(results)\n",
    "display.display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783c6d10",
   "metadata": {
    "editable": false,
    "id": "eaf02ea0",
    "lang": "fr",
    "tags": []
   },
   "source": [
    "### Entrez votre solution √† Q3C dans la cellule ci-dessous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516b95f",
   "metadata": {
    "editable": false,
    "id": "a22c170c",
    "lang": "en",
    "tags": []
   },
   "source": [
    "### Enter your answer to Q3C in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c984d32",
   "metadata": {
    "deletable": false,
    "id": "bbb930e4",
    "tags": [
     "user-answer-D3Q3C",
     "editable"
    ]
   },
   "outputs": [],
   "source": [
    "# Cr√©ation du tableau pour accumuler les r√©sultats\n",
    "# Creation of the table to accumulate the results\n",
    "results = {'Classifier':[\"DiscriminantANoyau\"],\n",
    "           'Range_lambda':[],\n",
    "           'Range_sigma':[],\n",
    "           'Best_lambda':[],\n",
    "           'Best_sigma':[],\n",
    "           'Error_train':[],\n",
    "           'Error_test':[],\n",
    "          }\n",
    "\n",
    "# *** TODO ***\n",
    "# Cr√©ez le jeu de donn√©es √† partir de la fonction make_moons, tel que demand√© dans l'√©nonc√©\n",
    "# N'oubliez pas de vous assurer que les valeurs possibles de y sont bel et bien dans -1 et 1, et non 0 et 1!\n",
    "# Create the dataset from the make_moons function, as requested in the statement\n",
    "# Remember to make sure that the possible values of y are in -1 and 1, not 0 and 1!\n",
    "# ******\n",
    "X, y = make_moons(ùëõ_ùë†ùëéùëöùëùùëôùëíùë†=800, ùëõùëúùëñùë†ùëí=0.3)\n",
    "y = numpy.where(y==0, -1, 1)\n",
    "\n",
    "# *** TODO ***\n",
    "# S√©parez le jeu de donn√©es en deux parts √©gales, l'une pour l'entra√Ænement et l'autre pour le test.\n",
    "# Separate the dataset into two equal parts, one for training and one for testing.\n",
    "# ******\n",
    "X1, X2, y1, y2 = train_test_split(X, y, train_size=0.5, random_state=96)\n",
    "\n",
    "_times.append(time.time())\n",
    "    \n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# param√®tre Lambda en la mettant dans \n",
    "# la liste de la variable range_lambda.\n",
    "# Specify the search range for the \n",
    "# parameter Lambda by putting it in \n",
    "# the list of the variable range_lambda.\n",
    "range_lambda = [10**n for n in range(-5, 6)]\n",
    "# ******\n",
    "\n",
    "results['Range_lambda'].append(range_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la plage de recherche pour le \n",
    "# param√®tre Sigma en la mettant dans \n",
    "# la liste de la variable range_sigma.\n",
    "# Specify the search range for the \n",
    "# parameter Sigma by putting it in \n",
    "# the list of the variable range_sigma.\n",
    "range_sigma = [round(0.1*n, 1) for n in range(1, 10)]\n",
    "# ******\n",
    "\n",
    "results['Range_sigma'].append(range_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Optimisez ici les param√®tres lambda et sigma de votre \n",
    "# classifieur en effectuant une recherche en grille.\n",
    "# Optimize here the lambda and sigma parameters of your \n",
    "# classifier by performing a grid search.\n",
    "# ******\n",
    "lambda_o, sigma_o = 0, 0\n",
    "s = -1\n",
    "\n",
    "for lambda_ in range_lambda:\n",
    "    for sigma in range_sigma:\n",
    "#         print(\"lambda=\", lambda_, \"sigma=\", sigma)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.5, random_state=69)\n",
    "        clf = DiscriminantANoyau(lambda_, sigma, verbose=False)\n",
    "        clf.fit(X_train, y_train)\n",
    "        tmp = clf.score(X_test, y_test)\n",
    "        if tmp > s:\n",
    "            lambda_o, sigma_o = lambda_, sigma\n",
    "            s = tmp\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# param√®tre Lambda en la mettant dans la\n",
    "# variable best_lambda en rempla√ßant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Lambda by putting it in the\n",
    "# variable best_lambda by replacing the 0.\n",
    "best_lambda = lambda_o\n",
    "# ******\n",
    "\n",
    "results['Best_lambda'].append(best_lambda)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez la valeur optimale pour le\n",
    "# param√®tre Sigma en la mettant dans la\n",
    "# variable best_sigma en rempla√ßant le 0.\n",
    "# Specify the optimal value for the\n",
    "# parameter Sigma by putting it in the\n",
    "# variable best_sigma by replacing the 0.\n",
    "best_sigma = sigma_o\n",
    "# ******\n",
    "\n",
    "results['Best_sigma'].append(best_sigma)\n",
    "\n",
    "# *** TODO ***\n",
    "# Une fois les param√®tres lambda et\n",
    "# sigma de votre classifieur optimis√©s,\n",
    "# cr√©ez une instance de ce classifieur\n",
    "# en utilisant ces param√®tres optimaux.\n",
    "# Once the lambda and sigma parameters of your classifier are optimized,\n",
    "# create an instance of this classifier\n",
    "# using these optimal parameters.\n",
    "# ******\n",
    "clf_o = DiscriminantANoyau(best_lambda, best_sigma, verbose=False)\n",
    "clf_o.fit(X1, y1)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indicate the error rate obtained on the training set in the variable\n",
    "# err_train variable by replacing the 0.\n",
    "err_train = 1 - clf_o.score(X1, y1)\n",
    "# ******\n",
    "\n",
    "results['Error_train'].append(err_train)\n",
    "\n",
    "# *** TODO ***\n",
    "# Indiquez le taux d'erreur obtenu sur le\n",
    "# jeu de test dans la variable \n",
    "# err_test en rempla√ßant le 0.\n",
    "# Indicate the error rate obtained on the\n",
    "# test set in the err_test variable by replacing the 0.\n",
    "err_test = 1 - clf_o.score(X2, y2)\n",
    "# ******\n",
    "\n",
    "results['Error_test'].append(err_test)\n",
    "\n",
    "# *** TODO ***\n",
    "# Cr√©ez ici une grille permettant d'afficher les r√©gions de d√©cision pour chaque classifieur\n",
    "# Indice : numpy.meshgrid pourrait vous √™tre utile ici\n",
    "# Par la suite, affichez les r√©gions de d√©cision dans la m√™me figure que les donn√©es de test.\n",
    "# Note : un pas de 0.02 pour le meshgrid est recommand√©\n",
    "# ******\n",
    "h = .02\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = numpy.meshgrid(numpy.arange(x_min, x_max, h), numpy.arange(y_min, y_max, h))\n",
    "\n",
    "pyplot.figure()\n",
    "\n",
    "colors = numpy.array([x for x in \"bgrcmyk\"])\n",
    "\n",
    "y_pred = clf_o.predict(numpy.c_[xx.ravel(), yy.ravel()]) \n",
    "y_pred = y_pred.reshape(xx.shape)\n",
    "        \n",
    "pyplot.contourf(xx, yy, y_pred, cmap=pyplot.cm.Paired, alpha=0.5)\n",
    "pyplot.scatter(X[:, 0], X[:, 1], cmap=pyplot.cm.Paired, c=colors[y])\n",
    "        \n",
    "pyplot.xlim(xx.min(), xx.max())\n",
    "pyplot.ylim(yy.min(), yy.max())\n",
    "\n",
    "# On affiche la figure\n",
    "_times.append(time.time())\n",
    "checkTime(TMAX_EVAL, \"Evaluation\")\n",
    "pyplot.show()\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "df = pandas.DataFrame(results)\n",
    "display.display(df)"
   ]
  }
 ],
 "metadata": {
  "PAX": {
   "userLang": "fr"
  },
  "celltoolbar": "",
  "jupytext": {
   "notebook_metadata_filter": "celltoolbar",
   "text_representation": {
    "extension": ".md",
    "format_name": "markdown",
    "format_version": "1.3",
    "jupytext_version": "1.11.4"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (PAX)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "fr",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
